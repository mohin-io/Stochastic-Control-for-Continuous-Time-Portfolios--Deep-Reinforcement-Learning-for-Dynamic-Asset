# DQN Agent Configuration

name: "DQN"
type: "discrete"

# Network architecture
hidden_dims:
  - 128
  - 64

# Learning parameters
learning_rate: 1.0e-4
gamma: 0.99  # Discount factor

# Exploration
epsilon_start: 1.0
epsilon_end: 0.01
epsilon_decay: 0.995

# Replay buffer
buffer_capacity: 10000
batch_size: 64

# Target network
target_update_freq: 10  # Update target network every N steps

# Prioritized replay (if using Prioritized DQN)
use_per: false
per_alpha: 0.6
per_beta_start: 0.4
per_beta_end: 1.0

# Double DQN
use_double_dqn: false

# Dueling architecture
use_dueling: false

# Noisy networks
use_noisy: false
